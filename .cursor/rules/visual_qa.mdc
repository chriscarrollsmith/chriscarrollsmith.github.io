---
description: Use a vision model to verify that UI changes visually enhance the website
alwaysApply: false
---

Simon Willison's `llm` tool is installed in this environment, and you can use it to interact with a vision model.

## Workflow (high-level)

1. Capture **baseline screenshots** for each viewport-height pane at the relevant breakpoints (e.g., desktop, mobile).
2. Capture **candidate screenshots** for the same panes and breakpoints after a visual change.
3. Run a **rubric-based comparison** in `llm` between baseline and candidate screenshots.
4. Adjust the frontend based on the feedback and re-run the comparison, aiming for strictly better scores on the affected panes and no regressions elsewhere.

Vision models can be sensitive to **file format, compression level, and crop**. Always keep **file format, encoding, viewport, breakpoint, and crop** consistent when comparing before/after shots.

## Basic usage of `llm`

- Attach screenshots with `-a` (e.g., `llm -a path/to/image.png "Question about image"`).
- Specify a schema for the output with the syntax, `--schema "variable_name type: Description"`.
- If desired for piping output to `jq` or JSON file for programmatic usage, use `--xl` (short for `--extract-last` fenced code block) to extract fenced json from the LLM's markdown response.

Note that you can attach multiple images with `-a`, and the AI model can distinguish them by sequence, but the model will not know the filenames.

## Standard rubric (0–2 × 5 metrics = 0–10 composite)

This section is a copy of `visual_qa/standard_rubric.md` and **must be kept in sync** with that file. The rubric consists of **five metrics**, each scored **0, 1, or 2**. Sum them into a **composite score from 0–10**. Separately score the baseline and candidate on the rubric, then compare the scores.

For all metrics:
- 0 = Fails this metric; clear problems.
- 1 = Acceptable; minor issues but generally OK.
- 2 = Exemplary; clearly strong and hard to improve without tradeoffs.

**Metrics**

1. Color cohesion & contrast
   - 0: Clashing or incoherent colors, poor contrast, or backgrounds that make content hard to perceive.
   - 1: Generally cohesive palette with adequate contrast, minor rough edges or slightly off tones.
   - 2: Harmonious, intentional palette with strong contrast where needed and no distracting clashes.

2. Text readability
   - 0: Text is hard to read (too small, low contrast, busy background, cramped line spacing).
   - 1: Text is readable but could be improved (slightly small, borderline contrast, or minor background interference).
   - 2: Text is very easy to read; sizes, weights, and line spacing feel deliberate and comfortable.

3. Layout & spacing
   - 0: Noticeable visual problems (overlaps, cramped sections, awkward gaps, misalignment, or clipping).
   - 1: Layout works, with minor alignment or spacing issues that a user would tolerate but a designer would notice.
   - 2: Clean, balanced composition with clear alignment, breathing room, and no obvious spacing issues.

4. Responsiveness / hierarchy preservation
   - 0: On the alternate breakpoint (e.g., mobile), important content feels lost, cramped, or re-ordered in a confusing way.
   - 1: The core hierarchy is preserved but some elements feel squeezed, de-emphasized, or visually awkward.
   - 2: The alternate breakpoint clearly preserves the hierarchy and intent of the primary layout while remaining comfortable to use.

5. Cohesion & wow factor
   - 0: The pane feels out of step with the rest of the site or visually flat in a way that detracts from the experience.
   - 1: Fits the site reasonably well but is unremarkable, or trades a bit of cohesion for local improvements.
   - 2: Feels cohesive with the rest of the site and has a clear “wow” or polish factor without breaking consistency.

The **composite score** is the sum of the five metric scores (0–10).

## Extending the standard rubric

The standard rubric is extensible; simply add 2 to the max composite score for each new metric. For example,

```bash
llm -a visual-qa/baseline/home-desktop.webp \
    "$(
      cat visual-qa/standard_rubric.md
    )

6. Embedded image crispness and quality
   - 0: An embedded image is blurry, pixelated, or otherwise low quality.
   - 1: Embedded images are generally acceptable, with minor potential issues.
   - 2: Embedded images are crisp and very high quality.

Score image on all six metrics using 0/1/2." \
    --schema "color_score int, text_readability_score int, layout_spacing_score int, responsiveness_hierarchy_score int, cohesion_wow_score int, embedded_image_quality_score int"
```

If baseline and candidate score the same, you can also submit both images to the vision model and ask whether the first candidate is **better, same, or worse** than the second, and ask for a short note on why.

You can also use `llm` for answering more focused questions, like, "Is the placement of the nav bar at the top of the page partially obscuring any page content?"
